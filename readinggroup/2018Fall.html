---
layout: default
---
{% include share_thumbnail.html page=page %}

<article class="feature-image">

	<header id="main" style="background-image: url('/assets/img/pexels/travel.jpeg'); background-position: 100% 0px, 0% center, center top;">
	    <h1 id="Reading+Group" class="title">
	        Reading Group
	    </h1>
	    
	  </header>
  <section class="post-content">
  {% if page.bootstrap %}
  <div class="bootstrap-iso">
  {% endif %}

	<h2 >2018 Fall</h2>
	<div align="center">
	<table>
	  <tr>
	    <th>#</th>
	    <th>Date</th>
	    <th>Topic</th>
	    <th>Presenter</th>
	    <th>Slides</th>
	    <th>Papers</th>
	  </tr>

	  <tr>
	    <td>1</td>
	    <td>10/30</td>
	    <td>Graph Neural Networks</td>
	    <td>Yunsheng Bai</td>
	    <td><a href="https://docs.google.com/presentation/d/1KWrjgYp-rpyOHZhL5eveTz0qpggpxriI3u4md6D7DaM/edit?usp=sharing">slides</a></td>
	    <td>1.Gated graph sequence neural networks. Li, Yujia, et al. ICLR (2016)<br> 
			2. How Powerful are Graph Neural Networks?. Xu, Keyulu, et al.  arXiv preprint arXiv:1810.00826 (2018)<br> 
			3. Graph Transformer (https://openreview.net/forum?id=HJei-2RcK7)
	  </tr>

	    <tr>
	    <td>2</td>
	    <td>11/6</td>
	    <td>Reasoning with Relational Data</td>
	    <td>Vivian Cheng</td>
	    <td><a href="https://drive.google.com/file/d/1ErbyXheNTOIA-qZNu7DfxU9GWoovplkP/view?usp=sharing">slides</a></td>
	    <td>1.TensorLog: A Differentiable Deductive Database (NIPS 2016) 
	    </td>
	  </tr>

	    <tr>
	    <td>3</td>
	    <td>11/13</td>
	    <td>Reasoning with Relational Data</td>
	    <td>Vivian Cheng</td>
	    <td><a href="https://drive.google.com/file/d/1ErbyXheNTOIA-qZNu7DfxU9GWoovplkP/view?usp=sharing">slides</a></td>
	    <td>2.Differentiable Learning of Logical Rules for Knowledge Base Reasoning (NIPS 2017) 
	    </td>
	  </tr>  

	  	<tr>
	    <td>4</td>
	    <td>11/20</td>
	    <td>Pretraining for NLP</td>
	    <td>Ziniu Hu</td>
	    <td><a href="https://drive.google.com/file/d/1D0TGRasJBj_cmi2XKCkhC62IPPSPIzmw/view?usp=sharing">slides</a></td>
	    <td>1. [Word2Vec] Distributed Representations of Words and Phrases and their Compositionality (NIPS' 13)<br>
			2. [ELMO] Deep contextualized word representations (NAACL' 18 Best Paper)<br>
			3. [Transformer] Attention Is All You Need (NIPS' 17)<br>
			4. [BERT] Bidirectional Encoder Representations from Transformers (NAACL'19 Best Paper) 
	    </td>
	  </tr>  




	</table>
	</div>





<!--   {% if page.bootstrap %}
  </div>
  {% endif %} -->
  </section>
    
</article>

